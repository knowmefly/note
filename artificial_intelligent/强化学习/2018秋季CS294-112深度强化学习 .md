---
title: 2018秋季CS294-112深度强化学习 
tags: 强化学习,AI研习社,加利福尼亚大学伯克利分校
renderNumberedHeading: true
grammar_cjkRuby: true
---
[课程地址](https://www.yanxishe.com/overseasCourse/30)
开始时间：2019年05月17日
结束时间：2020年02月

## 强化学习简介
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf)
**相关概念**
- 马尔可夫决策
- observation：全知状态
- policy：策略
- 强化学习算法（RL-Algorithm）
- Q-function
- value function

### 马尔可夫决策：
- 概念
	- state：状态
	- action：行动
	- reward：奖励
	- transition：转移算子（张量）

![马尔可夫过程](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578041604876.png)

### 无限视野情况（infinite horizon case）
- 状态-行动边缘分布（state-action marginal）：适用于有限时间，用于无限空间的公式推导![边缘分布情况](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581566107123.png)
- 稳态分布（station distribution）
![station distribution](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578194325741.png)
### 强化学习算法（RL algorithm）
![RL algorithm](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578194677724.png)

![Back prop algorithm](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578194978150.png)
- 随机系统如何定义条件期望
![如何定义条件期望函数](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578195634912.png)
![Q-function 、 Value function](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581577503032.png)
![Q-function and Value function](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581577688958.png)
### review
![review](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578196103106.png)
### 后续相关算法
![RL 相关 policy algorithm](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578196462898.png)
- 为什么会有不同算法
![many algorithm reason](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581580406083.png)
- 样本效率比较
![样本效率](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578196832685.png)
![不同算法样本效率比较](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581580616317.png)
- 效率和易用性比较![效率和易用性](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581580917871.png)
![效率和易用性比较](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581581052750.png)
- 假设
	- 全知状态
	- 片段式的
	- 连续性和光滑性
![一些假设的比较](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578197243681.png)
### 常用算法
![常用算法](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581581511524.png)

## 策略梯度简介
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf)
**相关概念**
- policy
- log替代
- S1到St视为常数
- 将平均奖励作为基准线

### 策略运算
- 用log函数去替代![实际使用策略](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581685828844.png)
![实际使用策略](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581686129283.png)
- 求解策略梯度![求解策略梯度](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581687263375.png)
- 最大似然估计![最大似然估计](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581687397284.png)
- 高斯策略![高斯策略](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581688654800.png)
- 梯度下降方式：对应状态好的行动概率逐渐增大，坏的行动概率逐渐减少![梯度下降方式](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581688776806.png)
- 梯度下降的缺陷：（neg很大，pos较小，样本较少）在样本不足时常量的存在会拔高总体的得分，拉平neg与pos的差别使分布趋于两者之间，而无法使概率尽可能的位于pos范围![缺陷](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581689249985.png)
- Review![review](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581689567279.png)
### 两个强化学习常用技巧
- 削减方差：（未来不会对过去有影响）将S1到St固定，只考虑St到St+1的概率分布来缩减方差![削减方差](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581690472471.png)
- 基准线：平均奖励可能不是最好的基准线，但它是相当有效的![基准线](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581690805522.png)
- 求最佳基准线：通过分析方差![方差分析](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581691219742.png)
- Review![Review](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581691357968.png)

### 在线策略和离线策略
- 策略梯度是在线策略![离线策略](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581691679919.png)
- 离线策略&重要样本![离线策略](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581692034418.png)
![重要样本](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581692218937.png)
![忽略某些权重](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581692544771.png)
![在状态动作下的边缘期望](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581692895467.png)
- 自动化分离求策略梯度![定义伪损失函数](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581693187265.png)
![tensorflow代码实现](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581693465532.png)
- 实际应用时的策略梯度![实际应用时](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581693623183.png)
- Review![Review](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581693735127.png)
- ISExample![示例](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581694015418.png)
- 自然策略梯度![自然策略梯度](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581694104730.png)

### 相关论文
![相关论文](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581694273794.png)

## Actor-Critic（评判家）算法简介
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-6.pdf)
**相关概念**
- 时间衰减因子
- 两个网络
- A批次输入，C对一定阶段A进行评判
- N步阶段

### 策略评估
![策略求解](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581746930149.png)
- 蒙特卡洛对价值函数的近似![蒙特卡洛](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581750700101.png)
- 更进以一步操作![Bootstrapped](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581751606831.png)

### Actor-Critic algorithm（演员评论家算法）
- 算法简介![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581752280938.png)
- 在无限模式下引入时间衰减因子（期望更早的获得奖励）![无限模式](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581752717710.png)
- 无限模式下的策略梯度![算法](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581753276634.png)
- 演员评论家算法在线模式衍生![在线](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581753796355.png)
- 架构设计：一般行动和评判采用两个网络；一个网络可以实现权重共享，但是两者之间通常方差差别很大![架构设计](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581754178994.png)
- 在线A-C算法实际使用：A采用并行的方式模拟批次输入，C对一定阶段的A的结果做评判，并行可同步也可不同步![实际使用](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581754407636.png)
- C状态相关的基线设置：同时实现无偏和小的方差![状态相关基线](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581754834199.png)
- 控制方差：动作相关基准线，先减去Q再加回来会使期望接近于零同时不会出错（等于零），再两次的近似中求得理想方差![控制方差](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581755373487.png)
- N步截断：会损失一些回报，但对整体影响不大且可以使方差减少![N步截断](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581755792063.png) 
- Generalized advantage estimation（集成优势估计器）：discount=variance reduction![GAE](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581756354593.png)
- Review ![Review](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581756463693.png)

### A-C相关论文
![A-C相关论文](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581757001919.png)

## 价值函数介绍
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-7.pdf)
**相关概念**
- 拟合Q迭代，无需知道状态转移矩阵
- 拟合Q迭代不保证收敛，最佳选项和Q值下降无直接关系

### 价值函数 V or Q-V，策略π
- 策略迭代![策略迭代](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582592407164.png)
- 动态规划![动态规划](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582593479359.png)
- 策略迭代与动态规划结合![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582593753191.png)
- 简化的动态规划![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582594024637.png)
- 拟合值迭代![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582594283990.png)
- 使用Q-function替换状态转移矩阵做拟合值迭代![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582594668843.png)
- 策略迭代->拟合值迭代->拟合Q迭代![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582595081030.png)
- 拟合Q迭代步骤![步骤](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582597112607.png)
- 拟合Q迭代（无策略方式）![算法简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582597387917.png)
- 拟合Q迭代在优化什么：提升表格情况下的策略；最小贝尔曼方差。Q﹡当贝尔曼方差为零时说明函数满足等式![拟合Q迭代优化](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582597883702.png)
- 在线Q-learning algorithm![算法简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582598093774.png)
- Q-learning的探索：ε-贪婪、玻尔兹曼![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582598529885.png)
- Review![Q-learning在RL的使用](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582598769830.png)

### 价值函数补充
- 值方法学习理论：通过值来寻找最佳策略![理论简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582599490898.png)
- 不在表格的值学习方法：V<- πBV![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582600061079.png)
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582600248023.png)
- 拟合Q迭代![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582600364611.png)
- 拟合Q迭代缺陷：无法保证收敛 最终目标无法通过梯度下降确定获得![缺陷（退步）](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582600555591.png)
- 坏消息：bootstrap并不是总有效![sad corollary](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582600724222.png)
- Review![课程回顾](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582600894170.png)

## 深度强化学习和Q-function
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf)
**相关概念**
- d
- das

### Q-function缺陷
- 忽略方程式一部分求解不是全部；忽略批次输入间的顺序关系![缺陷问题](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582853333095.png)
- 过度拟合陷入局部区域，无法保证输入数据之间的强相关性![缺陷问题](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582853602363.png)
- 缓存回放（fitted Q-learning）：需要大量的随机数据去拟合概率空间；后续可加入与真实世界的互动![缓存回放](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582853897382.png)
![动画示意](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582854135219.png)
![Q-learning结合缓存回放](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582855853683.png)
### Q-learning的使用
- 类似有监督的回归任务![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582856444547.png)
- 