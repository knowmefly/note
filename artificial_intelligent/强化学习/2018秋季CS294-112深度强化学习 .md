---
title: 2018秋季CS294-112深度强化学习 
tags: 强化学习,AI研习社,加利福尼亚大学伯克利分校
renderNumberedHeading: true
grammar_cjkRuby: true
---
[课程地址](https://www.yanxishe.com/overseasCourse/30)
开始时间：2019年05月17日
结束时间：2020年02月

## 强化学习简介
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf)
**相关概念**
- 马尔可夫决策
- observation：全知状态
- policy：策略
- 强化学习算法（RL-Algorithm）
- Q-function
- value function

### 马尔可夫决策：
- 概念
	- state：状态
	- action：行动
	- reward：奖励
	- transition：转移算子（张量）

![马尔可夫过程](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578041604876.png)

### 无限视野情况（infinite horizon case）
- 状态-行动边缘分布（state-action marginal）：适用于有限时间，用于无限空间的公式推导![边缘分布情况](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581566107123.png)
- 稳态分布（station distribution）
![station distribution](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578194325741.png)
### 强化学习算法（RL algorithm）
![RL algorithm](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578194677724.png)

![Back prop algorithm](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578194978150.png)
- 随机系统如何定义条件期望
![如何定义条件期望函数](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578195634912.png)
![Q-function 、 Value function](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581577503032.png)
![Q-function and Value function](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581577688958.png)
### review
![review](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578196103106.png)
### 后续相关算法
![RL 相关 policy algorithm](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578196462898.png)
- 为什么会有不同算法
![many algorithm reason](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581580406083.png)
- 样本效率比较
![样本效率](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578196832685.png)
![不同算法样本效率比较](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581580616317.png)
- 效率和易用性比较![效率和易用性](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581580917871.png)
![效率和易用性比较](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581581052750.png)
- 假设
	- 全知状态
	- 片段式的
	- 连续性和光滑性
![一些假设的比较](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578197243681.png)
### 常用算法
![常用算法](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581581511524.png)

## 策略梯度简介
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf)
**相关概念**
- policy
- log替代
- S1到St视为常数
- 将平均奖励作为基准线

### 策略运算
- 用log函数去替代![实际使用策略](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581685828844.png)
![实际使用策略](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581686129283.png)
- 求解策略梯度![求解策略梯度](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581687263375.png)
- 最大似然估计![最大似然估计](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581687397284.png)
- 高斯策略![高斯策略](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581688654800.png)
- 梯度下降方式：对应状态好的行动概率逐渐增大，坏的行动概率逐渐减少![梯度下降方式](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581688776806.png)
- 梯度下降的缺陷：（neg很大，pos较小，样本较少）在样本不足时常量的存在会拔高总体的得分，拉平neg与pos的差别使分布趋于两者之间，而无法使概率尽可能的位于pos范围![缺陷](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581689249985.png)
- Review![review](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581689567279.png)
### 两个强化学习常用技巧
- 削减方差：（未来不会对过去有影响）将S1到St固定，只考虑St到St+1的概率分布来缩减方差![削减方差](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581690472471.png)
- 基准线：平均奖励可能不是最好的基准线，但它是相当有效的![基准线](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581690805522.png)
- 求最佳基准线：通过分析方差![方差分析](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581691219742.png)
- Review![Review](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581691357968.png)

### 在线策略和离线策略
- 策略梯度是在线策略![离线策略](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581691679919.png)
- 离线策略&重要样本![离线策略](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581692034418.png)
![重要样本](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581692218937.png)
![忽略某些权重](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581692544771.png)
![在状态动作下的边缘期望](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581692895467.png)
- 自动化分离求策略梯度![定义伪损失函数](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581693187265.png)
![tensorflow代码实现](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581693465532.png)
- 实际应用时的策略梯度![实际应用时](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581693623183.png)
- Review![Review](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581693735127.png)
- ISExample![示例](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581694015418.png)
- 自然策略梯度![自然策略梯度](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581694104730.png)

### 相关论文
![相关论文](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581694273794.png)

## Actor-Critic（评判家）算法简介
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-6.pdf)
**相关概念**
- 时间衰减因子
- 两个网络
- A批次输入，C对一定阶段A进行评判
- N步阶段

### 策略评估
![策略求解](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581746930149.png)
- 蒙特卡洛对价值函数的近似![蒙特卡洛](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581750700101.png)
- 更进以一步操作![Bootstrapped](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581751606831.png)

### Actor-Critic algorithm（演员评论家算法）
- 算法简介![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581752280938.png)
- 在无限模式下引入时间衰减因子（期望更早的获得奖励）![无限模式](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581752717710.png)
- 无限模式下的策略梯度![算法](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581753276634.png)
- 演员评论家算法在线模式衍生![在线](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581753796355.png)
- 架构设计：一般行动和评判采用两个网络；一个网络可以实现权重共享，但是两者之间通常方差差别很大![架构设计](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581754178994.png)
- 在线A-C算法实际使用：A采用并行的方式模拟批次输入，C对一定阶段的A的结果做评判，并行可同步也可不同步![实际使用](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581754407636.png)
- C状态相关的基线设置：同时实现无偏和小的方差![状态相关基线](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581754834199.png)
- 控制方差：动作相关基准线，先减去Q再加回来会使期望接近于零同时不会出错（等于零），再两次的近似中求得理想方差![控制方差](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581755373487.png)
- N步截断：会损失一些回报，但对整体影响不大且可以使方差减少![N步截断](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581755792063.png) 
- Generalized advantage estimation（集成优势估计器）：discount=variance reduction![GAE](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581756354593.png)
- Review ![Review](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581756463693.png)

### A-C相关论文
![A-C相关论文](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581757001919.png)

## 价值函数介绍
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-7.pdf)
**相关概念**
- 拟合Q迭代，无需知道状态转移矩阵
- 拟合Q迭代不保证收敛，最佳选项和Q值下降无直接关系

### 价值函数 V or Q-V，策略π
- 策略迭代![策略迭代](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582592407164.png)
- 动态规划![动态规划](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582593479359.png)
- 策略迭代与动态规划结合![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582593753191.png)
- 简化的动态规划![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582594024637.png)
- 拟合值迭代![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582594283990.png)
- 使用Q-function替换状态转移矩阵做拟合值迭代![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582594668843.png)
- 策略迭代->拟合值迭代->拟合Q迭代![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582595081030.png)
- 拟合Q迭代步骤![步骤](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582597112607.png)
- 拟合Q迭代（无策略方式）![算法简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582597387917.png)
- 拟合Q迭代在优化什么：提升表格情况下的策略；最小贝尔曼方差。Q﹡当贝尔曼方差为零时说明函数满足等式![拟合Q迭代优化](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582597883702.png)
- 在线Q-learning algorithm![算法简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582598093774.png)
- Q-learning的探索：ε-贪婪、玻尔兹曼![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582598529885.png)
- Review![Q-learning在RL的使用](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582598769830.png)

### 价值函数补充
- 值方法学习理论：通过值来寻找最佳策略![理论简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582599490898.png)
- 不在表格的值学习方法：V<- πBV![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582600061079.png)
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582600248023.png)
- 拟合Q迭代![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582600364611.png)
- 拟合Q迭代缺陷：无法保证收敛 最终目标无法通过梯度下降确定获得![缺陷（退步）](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582600555591.png)
- 坏消息：bootstrap并不是总有效![sad corollary](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582600724222.png)
- Review![课程回顾](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582600894170.png)

## 深度强化学习和Q-function
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-8.pdf)
**相关概念**
- 缓存回放B
- DQN
- 

### Q-function缺陷
- 忽略方程式一部分求解不是全部；忽略批次输入间的顺序关系![缺陷问题](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582853333095.png)
- 过度拟合陷入局部区域，无法保证输入数据之间的强相关性![缺陷问题](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582853602363.png)
- 缓存回放（fitted Q-learning）：需要大量的随机数据去拟合概率空间；后续可加入与真实世界的互动![缓存回放](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582853897382.png)
![动画示意](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582854135219.png)
![Q-learning结合缓存回放](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582855853683.png)
### Q-learning的使用
- 类似有监督的回归任务![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582856444547.png)
- 经典DQN algorithm![算法简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582856917274.png)
- 可以选择的目标网络：更新时更依赖前一状态![示意](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582857445081.png)
- 更全面的观点：Online Q-learning、DQN、Fitted Q-iteration![示意](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582860711115.png)
-  过度评估：为了去做两个区别较大的两个状态的转移![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582862545164.png)
-  Double Q-learning：使用两个Q-function避免噪声二次影响；实际使用不用构造两个网络，用上一状态网络去做评估![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582863141471.png)
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582863381200.png)
- N-step回传方法：批次回传减少偏差；更快的学习；只有在在线策略学习才有效![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582868610665.png)
- 连续动作下的Q-learning![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582868836167.png)
- （1）随机优化下的Q-learning![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582869082689.png)
- （2）NAF：Normalized Advantage Functions![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582869318071.png)
- （3）DDPG：接近最大化；使用另外网构建Qfai和μθ![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582869746525.png)
- 常见的Q-learning建议做法：简单测试；大的缓存回放；很花时间，需要有耐心；开始设定较高的ε梯度下降![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582870273088.png)
- 先进的关于Q-learning的做法：更换loss函数；Double Q-learning在实现上会有很大帮助，简单没有负面作用；N-step也有帮助，但会有负面作用；探索规划、学习率，Adam会有帮助；多试几个随机种子，它们对结果影响很大。

![先进的建议](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1582870690845.png)

## 高级策略梯度
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-9.pdf)

**相关概念**
- 自然梯度和信任区间
- 拉格朗日乘数法
- 泰勒展开
- 自然梯度
###  回顾
- 策略梯度--梯度迭代推导
![公式推导](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586828479336.png)
### 分布边界改变
- 分布边界改变
![假设简化](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586829427553.png)
![假设简化](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586832008178.png)
![enter description here](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586849791792.png)
- 一些更方便的边界
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586852123415.png)
### 策略梯度求解
- 拉格朗日乘数法
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586853896882.png)
- 如何求最优解
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586854184258.png)
![方程简化](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586854357754.png)
- 如何使用梯度下降
![使用梯度下降](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586854636550.png)
![矩阵近似和泰勒展开](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586854882494.png)
![对约束条件进行完善](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586855261993.png)
### 这么做存在的问题
- 自然梯度需要迭代很长时间；但初始步长参数影响不大
![策略梯度与自然梯度](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586855993686.png)
### 自然梯度方法相关
![方法相关](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586856317683.png)
### review
![review](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586856412143.png)

## 最优控制和规划
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-10.pdf)

**相关概念**
- 开环与闭环
- 动态规则
- 基于模型
- 射靶法
- 交叉熵（CEM）
- 蒙特卡洛搜索树
- 高斯分布
- LQR（linear quadratic regulator）线性二次型调节器
- DDP 微分动态规划

### 课程概要
![课程概要](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586914742934.png)

### 如何得到控制策略
- 规则已知的控制方法
![规则已知](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586914694690.png)
- 基于模型
![介绍](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586915102645.png)
- 随机开环方式的控制策略（受限于维度；有可能陷入局部优化）
![介绍](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586915542810.png)
![实现方法：guess&check](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586916160364.png)
- 交叉熵方法（CEM，基于分布概率）
![介绍](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586917210296.png)
- 蒙特拉洛搜索树（离散情况；在进行一段时间后使用随机策略）
![介绍](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586917966566.png)
![介绍](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586918043517.png)
![方法实现](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586918955171.png)

### 连续控制于注入优化
- 射靶法和收集发（影响所有；影响局部）
![介绍](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586933961427.png)
- 线性方式：LQR
![公式](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586934392386.png)
![公式](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586934593735.png)
![公式](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586934737499.png)
![公式](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586935012721.png)
![伪代码实现](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586935285184.png)
- 随机动态（用高斯概率拟合随机元素，实际是闭环控制）
![介绍](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586935685273.png)
- 非线性方式（DDP）
![介绍](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586935886774.png)
![公式](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586935965523.png)
![DDP伪代码](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586936133499.png)
![有效性证明（基于牛顿法）](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586936219977.png)
![原始方程](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586936361152.png)
![伪代码](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1586936538641.png)

## 基于模型的强化学习
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-11.pdf)
**相关概念**
- LQR
- 高斯方法
- KL散度
- DGD对偶梯度下降

### 课程概要
![概要](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587086308142.png)
### 基于模型的不同程度
- version0.5（仅基于原始数据）
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587087614982.png)
- version1.0（将新生成的s0，a，s1加入到训练集进行迭代）
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587087310758.png)
- version1.5（先进行决策评估后再采取行为）
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587087724664.png)
![如何进行规划](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587087848337.png)
- version2.0（使用神经网络进行决策；需解决梯度爆炸和梯度消失问题）
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587088407716.png)
- 不同强化学习版本的总结
![summary](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587088629863.png)
- 高斯方法、神经网络和其他方法的比较
![各方法比较](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587089446572.png)
### local models
- LQR拟合
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587107787434.png)
![实现步骤](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587107881477.png)
- 基于模型不同版本控制执行对比
![对比](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587108360204.png)
![2.0加入噪声](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587108531129.png)
- 如何去拟合动态变化
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587109668319.png)
- 基于KL散度对动态边界进行限制
![公式推导](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587110425769.png)
- 对偶梯度下降（DGD）
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587110715427.png)
![推导](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587110830629.png)
![推导](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587110892693.png)
- 结合DGD(对偶梯度下降)的LQR
![实现步骤](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587111157613.png)
![步骤](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587111234448.png)
![步骤](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587111308937.png)
- 置信区间和轨迹分布
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587111393747.png)

## 高级强化学习和图像处理应用
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-12.pdf)
**相关概念**
- 模型不确定性
- 贝叶斯网络
- bootstrap ensemble 集成自举
- 

### 课程概要
![课程概要](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587263307833.png)

### 防止过拟合
- 构建两个策略，一个是高斯策略，一个不是高斯策略，非高斯策略去防止高斯策略偏向异常区域
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587264199339.png)
![示例](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587264419278.png)
![告诫](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587264695913.png)
- 不确定性如何存在
![介绍](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587265379398.png)
- 评估模型的不确定性
![方法](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587265599616.png)
- 贝叶网络简介
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587265842237.png)
- bootstrap ensemble 集成自举
![介绍](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587266424658.png)
- 如何在不确定下进行规划
![步骤](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587267026745.png)
- 力矩匹配
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587267287575.png)
- 推荐读物
![读物](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587267386719.png)

### 图像处理应用
- 样例步骤介绍（对图像信息进行采样，进行强化学习）
![介绍](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587268217516.png)
![评价](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587268296861.png)
- 另一种设计（加入softmax对结果取幂）
![介绍](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587268495958.png)
- 预测多种变量
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587269421455.png)

## 基于模型的的强化学习和策略学习
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-13.pdf)
**相关概念**
- 配点法
- 增广拉格朗日算法
- GPS 引导策略搜索

### 课程概要
![课程概要](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587431802733.png)

### 基于模型的闭环策略学习
- 存在的问题（起始点影响较大；存在梯度爆炸与梯度消失问题）
![简述](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587432821246.png)
- 配点法
![简述](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587433118505.png)
- 添加约束条件使用对偶梯度法求解
![步骤](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587433467384.png)
- 引入增广拉格朗日算法
![介绍](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587433706947.png)
- 实现步骤
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587433985402.png)
![总结](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587434166577.png)
- 策略搜索步骤一般性方法
![实现步骤](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587434397454.png)
![最终步骤](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587434554485.png)
- 多轨迹学习
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587435545857.png)
- GPS引导策略搜索
![介绍](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587435799932.png)
- 对可能选择的动作进行筛选，防止回报函数过大
![简介](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587438594995.png)
- DAgger和GPS的对比
![对比](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587438788453.png)

### 使用模型的非模型优化 
- 为什么选择模仿学习
![模仿学习的原因](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587438967690.png)
- Dyna（在线的Q-Learning）
![步骤](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587439330444.png)
- Dyna泛化版本的基于模型的强化学习（可以获得基于模型以外的选择）
![步骤](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1587439854840.png)