---
title: 2018秋季CS294-112深度强化学习 
tags: 强化学习 AI研习社 加利福尼亚大学伯克利分校
renderNumberedHeading: true
grammar_cjkRuby: true
---
[课程地址](https://www.yanxishe.com/overseasCourse/30)
开始时间：2019年05月17日
结束时间：2020年02月

## 强化学习简介
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-4.pdf)
**相关概念**
- 马尔可夫决策
- observation：全知状态
- policy：策略
- 强化学习算法（RL-Algorithm）
- Q-function
- value function

### 马尔可夫决策：
- 概念
	- state：状态
	- action：行动
	- reward：奖励
	- transition：转移算子（张量）

![马尔可夫过程](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578041604876.png)

### 无限视野情况（infinite horizon case）
- 状态-行动边缘分布（state-action marginal）：适用于有限时间，用于无限空间的公式推导![边缘分布情况](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581566107123.png)
- 稳态分布（station distribution）
![station distribution](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578194325741.png)
### 强换学习算法（algorithm）
![RL algorithm](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578194677724.png)

![Back prop algorithm](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578194978150.png)
- 随机系统如何定义条件期望
![如何定义条件期望函数](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578195634912.png)
![Q-function 、 Value function](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581577503032.png)
![Q-function and Value function](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581577688958.png)
### review
![review](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578196103106.png)
### 后续相关算法
![RL 相关 policy algorithm](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578196462898.png)
- 为什么会有不同算法
![many algorithm reason](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581580406083.png)
- 样本效率比较
![样本效率](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578196832685.png)
![不同算法样本效率比较](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581580616317.png)
- 效率和易用性比较![效率和易用性](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581580917871.png)
![效率和易用性比较](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581581052750.png)
- 假设
	- 全知状态
	- 片段式的
	- 连续性和光滑性
![一些假设的比较](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1578197243681.png)
### 常用算法
![常用算法](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581581511524.png)

## 策略梯度简介
[课程PPT](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf)
**相关概念**
- policy
- 

### 策略运算
- 用log函数去替代![实际使用策略](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581685828844.png)
![实际使用策略](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581686129283.png)
- 求解策略梯度![求解策略梯度](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581687263375.png)
- 最大似然估计![最大似然估计](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581687397284.png)
- 高斯策略![高斯策略](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581688654800.png)
- 梯度下降方式：对应状态好的行动概率逐渐增大，坏的行动概率逐渐减少![梯度下降方式](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581688776806.png)
- 梯度下降的缺陷：（neg很大，pos较小）在样本不足时常量的存在会拔高总体的得分，拉平neg与pos的差别使分布趋于两者之间，而无法使概率尽可能的位于pos范围![缺陷](https://gitee.com/knowmefly/little_book_maker/raw/master/小书匠/1581689249985.png)