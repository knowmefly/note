# 神经网络作为通用逼近器
**sigmoid**
通过神经网络可以模拟任意的布尔运算与多项式运算。

神经网络模拟情况的好坏取决于网络的深度和宽度。如果只用一层的话需要的节点数将会呈指数级增长，因此我们会用多层网络来进行模拟。

同时在网络的训练过程中在每一层的传递时需要考虑信息的损失，sigmoid激活函数会损失两端的信息，造成失真现象，常用于最后一层不需要回传信息时。其余层多用ReLU函数进行激活保留各层信息。
# 训练神经网络
#### 感知机
感知机--找到一个线性超平面区分正负样本

线性平面能够分开所有样本，并使所以样本距其最远。权重W是这个 超平面的法线，它会尽可能的指向正样本背离负样本，经过反复迭代找出最适位置。偏置b是为了找到所有样本中间位置。

只有前向传播寻找复杂样本的分界线将会是一个NP问题。所以为了解决问题引入了反向传播，同时为了使反向传播可导使用sigmoid做激活函数，又因为多数样本存在与x的中间位置，sigmoid对两边数据的区分度高，对中间位置区分度较低，所以给予出现次数多的样本更高的权重，出现次数少的样本较低权重。相当于求函数的概率期望。

# 反向传播
WtX = |W|x|X|cosb

两个向量的点乘，在计算时采用线性代数的运算法则行向量W乘以列向量X 得出两个向量之间的结果。

找到整个函数的最小值，考虑一阶导数与二阶导数。实际无法一次求得，采用反复迭代的方式求得结果。即函数导数为0点。每次前进方向都为函数导数的反方向取一定的步长。

最后利用交叉熵函数求预测结果y与标定d的相关回传梯度去修正W与b。两个结果之间的相关度使用散度来表示。

# 网络的收敛性
网络利用偏导数和散度反向计算前一轮W+b产生的误差。利用链式求导反向计算，前一层利用后一层回传的导数来计算自己的偏差，只需一次计算不用反复计算。

梯度计算与感知机的区别

梯度计算异常点对其影响不大，有可能出现误分情况。感知机会对每一个类别进行区分，异常点对其影响很大，计算过程可能会较大。

梯度下降在样本大时会产生多种鞍点容易陷入局部最优。在链式过长时可能梯度消失，softmax可能会产生梯度爆炸。